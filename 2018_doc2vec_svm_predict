#coding: utf-8
# python version: 2.7
# date:2018.5.24

import csv
import re
from sklearn.svm import SVC
from sklearn import metrics
from sklearn.model_selection import KFold
from gensim.models.doc2vec import Doc2Vec,TaggedDocument
import numpy as np

f = open("/home/iiip/Quincy/personality/data/essays.csv","r")
csv_file = csv.reader(f)

def clean_str(string):
    string = re.sub(r"[^A-Za-z0-9,!?\'\`]", " ", string)
    string = re.sub(r"\'s", " \'s ", string)
    string = re.sub(r"\'ve", " have ", string)
    string = re.sub(r"n\'t", " not ", string)
    string = re.sub(r"\'re", " are ", string)
    string = re.sub(r"\'d", " would ", string)
    string = re.sub(r"\'ll", " will ", string)
    string = re.sub(r",", " , ", string)
    string = re.sub(r"!", " ! ", string)
    string = re.sub(r"\(", " ( ", string)
    string = re.sub(r"\)", " ) ", string)
    string = re.sub(r"\?", " \? ", string)
    string = re.sub(r'\s{2,}', ' ', string)
    return string.strip().lower()

data,train,test = [],[],[]  # 放特征向量
label_name = []

for line in csv_file:
    if csv_file.line_num == 1:
        label_name = line[2:]
        continue   # 跳过第一行
    authid = line[0]
    content = line[1]
    one_text_label = [line[2],line[3],line[4],line[5],line[6]] # 五个标签
    content = clean_str(content)
    data.append(TaggedDocument(content.split(),one_text_label))



# model.save("/home/iiip/Quincy/personality/personality-detection-master/doc2vec_model_200.model")
# model = Doc2Vec.load("/home/iiip/Quincy/personality/personality-detection-master/doc2vec_model_200.model")

def metrics_result(actual, predict):
    # print '#精度:{0:.3f}'.format(metrics.precision_score(actual, predict,average='weighted'))
    # print '#召回:{0:0.3f}'.format(metrics.recall_score(actual, predict,average='weighted'))
    # print '#f1-score:{0:.3f}'.format(metrics.f1_score(actual, predict,average='weighted'))   #输出是字符串
    accuracy = round(metrics.precision_score(actual, predict,average='weighted'),3) #round() 第二个参数为精确小数点后几位
    recall = round(metrics.recall_score(actual, predict,average='weighted'),3)
    f1 = round(metrics.f1_score(actual, predict,average='weighted'),3)
    return accuracy,recall,f1

kf = KFold(n_splits=10)
data_array = np.array(data)
result = {"cEXT":[],"cNEU":[],"cAGR":[],"cCON":[],"cOPN":[]}
for i in range(10): # 训练10次特征模型
    model = Doc2Vec(data, size=200, workers=8)
    model.train(data, total_examples=model.corpus_count, epochs=100)
    for number in range(len(label_name)):  #针对每一个标签
        ave_accuracy = 0
        ave_recall = 0
        ave_f1 = 0

        for train_index,test_index in kf.split(data): # 10-fold
            print("done"),
            train,test = data_array[train_index].tolist(),data_array[test_index].tolist()

            train_X,train_y1 = [],[]
            test_X,test_y1 = [],[]


            for id in xrange(len(train)):  # xrange是生成器，减小内存消耗
                infer_vector = model.infer_vector(train[id][0])
                train_X.append(infer_vector)
                train_y1.append(train[id][1][number]) #train[id][1][n] 第id个文档的第n个标签

            for id in xrange(len(test)):
                infer_vector = model.infer_vector(test[id][0])
                test_X.append(infer_vector)
                test_y1.append(test[id][1][number])


            clf = SVC(kernel="linear")
            clf.fit(train_X,train_y1)
            y_predict = clf.predict(test_X)

            accuracy,recall,f1 = metrics_result(test_y1,y_predict)
            ave_accuracy+=accuracy
            ave_recall+=recall
            ave_f1+=f1

        print
        print label_name[number]
        print "ave_accuracy: ",ave_accuracy/10
        print "ave_recall: ",ave_recall/10
        print "ave_f1: ",ave_f1/10
        result[label_name[number]].append(accuracy)

with open("/home/iiip/Quincy/personality/personality-detection-master/10 Fold", "a") as f:
    get_average = lambda x:1.0*sum(x)/len(x)
    for label in result:
        average = get_average(result[label])
        f.write(label,": ")
        for i in result[label]:
            f.write(i," ")
        f.write(average,"\n")
